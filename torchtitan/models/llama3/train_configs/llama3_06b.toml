[job]
dump_folder = "./outputs"
description = "Llama 3 0.6B training"

[profiling]
enable_profiling = true
save_traces_folder = "profile_trace"
profile_freq = 100

[metrics]
log_freq = 1
enable_wandb = true


[model]
name                = "llama3"
hidden_size         = 1536      # 24 layers × 1536 dim ≃ 600 M params
num_attention_heads = 12
num_layers          = 24
tokenizer_path      = "./assets/tokenizer/Llama-3.2-1B/original/tokenizer.model"

[training]
local_batch_size = 8
grad_accum       = 8              # 8 (accum) × 8 (local) × 1 (DP) = 512 tokens
seq_len          = 4096
steps            = 2383           # 20 B tokens / (512×4096)
compile          = true
eval_interval    = 1000
max_norm = 1.0  # grad norm clipping
dataset = "c4"


[parallelism]
data_parallel_replicate_degree = 1
data_parallel_shard_degree = -1
tensor_parallel_degree = 1
pipeline_parallel_degree = 1
context_parallel_degree = 1

[float8]
enable_float8_linear          = true
enable_fsdp_float8_all_gather = true

[checkpoint]
enable_checkpoint = true
folder            = "checkpoint"
interval          = 500
async_mode        = "async"
export_dtype      = "bfloat16"


[activation_checkpoint]
mode = "selective"  # ["none", "selective", "full"]
selective_ac_option = "op"  # "int" = ac every positive int layer or 'op', ac based on ops policy